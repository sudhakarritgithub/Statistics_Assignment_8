{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a7a8a9f6-cbd2-440d-aba4-e586b0bcab3d",
   "metadata": {},
   "source": [
    "                                                        STATISTICS ASSIGNMENT - 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93794613-9c75-4cca-8dd3-373d9ab9af01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "89b3af49-d2d0-4b83-b545-e299c0aae030",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "587028e8-c062-4e18-ad60-c67b1cedda4b",
   "metadata": {},
   "source": [
    "Ques.1  Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6380830c-5b34-41f0-8132-ce156d37a3fb",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical technique used to compare means of two or more groups to determine if there are statistically significant differences among them. It relies on several assumptions for its validity. Here are the key assumptions required to use ANOVA and examples of violations that could impact the validity of the results:\n",
    "\n",
    "Assumptions of ANOVA:\n",
    "\n",
    "Independence of Observations:\n",
    "Each data point (observation) should be independent of the others. This means that the outcome of one observation should not be influenced by the outcome of another observation.\n",
    "Violation Example: In a repeated measures design where the same subjects are measured under different conditions, observations within the same subject may not be independent. For example, if subjects are measured at different time points, measurements at different times may be correlated.\n",
    "\n",
    "Normality:\n",
    "The residuals (the differences between observed and predicted values) should be normally distributed. This assumption pertains to the distribution of errors rather than the distribution of the original data itself.\n",
    "Violation Example: If the residuals are not normally distributed, it can lead to incorrect inference about the significance of group differences. For instance, if the residuals are skewed or heavily tailed, ANOVA results may be biased.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity):\n",
    "The variances of the groups being compared should be approximately equal (homogeneous). This means that the spread or dispersion of scores in each group should be similar across all groups.\n",
    "Violation Example: If one group has significantly larger variance (spread of scores) compared to others, it can lead to unreliable F-statistics and inflated Type I error rates. For instance, if one treatment group shows much higher variability in outcomes compared to others, ANOVA may incorrectly suggest significant differences between groups.\n",
    "\n",
    "Interval or Ratio Data:\n",
    "The dependent variable should be measured on an interval or ratio scale. ANOVA assumes that the data can be meaningfully added and subtracted.\n",
    "Violation Example: Using ANOVA on categorical or ordinal data without appropriate transformation can lead to misleading results. For example, using ANOVA on Likert scale responses without verifying interval properties could violate this assumption.\n",
    "\n",
    "Impact of Violations:\n",
    "Type I Error Rate: Violations of these assumptions can inflate the Type I error rate (false positive rate), leading to incorrect conclusions about significant differences between groups.\n",
    "Power of the Test: Violations can reduce the power of the test, making it less likely to detect true differences between groups when they actually exist.\n",
    "Bias in Estimates: Violations can bias estimates of group differences, leading to unreliable and misleading interpretations of study findings.\n",
    "\n",
    "Handling Violations:\n",
    "\n",
    "When assumptions of ANOVA are violated, several approaches can be considered:\n",
    "\n",
    "Transforming Data: Transforming the data (e.g., logarithmic or square root transformation) may help to meet assumptions such as normality or variance homogeneity.\n",
    "\n",
    "Using Non-parametric Tests: If assumptions cannot be met even after transformation, non-parametric alternatives (e.g., Kruskal-Wallis test for independent groups) can be used, which do not require assumptions about the distribution of the data.\n",
    "\n",
    "Robust ANOVA Methods: Some ANOVA methods are robust to violations of normality and/or homogeneity of variance assumptions. However, they may require larger sample sizes to maintain accurate Type I error rates.\n",
    "\n",
    "Resampling Methods: Methods such as bootstrapping can provide valid inference without relying on specific distributional assumptions, although they require computational resources."
   ]
  },
  {
   "cell_type": "raw",
   "id": "da4860f1-781c-4e74-87d6-a2391e5d0c82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "553ef73c-cbf0-4fcb-8f74-ffc073721507",
   "metadata": {},
   "source": [
    "Ques.2 What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8cff3fe-e283-4d31-8d8e-3f094fcb6b09",
   "metadata": {},
   "source": [
    "One-Way ANOVA:\n",
    "Usage: One-Way ANOVA is used when you have one categorical independent variable (factor) with two or more levels (groups), and you want to compare the means of a single continuous dependent variable across those groups.\n",
    "Example: Comparing the mean test scores of students across different educational levels (e.g., high school, college, university).\n",
    "\n",
    "Two-Way ANOVA:\n",
    "Usage: Two-Way ANOVA extends the one-way ANOVA by allowing for the simultaneous comparison of the effects of two categorical independent variables (factors) on a single continuous dependent variable. It assesses not only the main effects of each factor but also whether there is an interaction effect between the factors.\n",
    "Example: Examining the effects of both gender and treatment type on blood pressure change. Here, gender (male vs. female) and treatment type (drug A vs. drug B) are the two independent variables.\n",
    "\n",
    "Repeated Measures ANOVA:\n",
    "Usage: Repeated Measures ANOVA is used when you have one group of participants measured on the same continuous dependent variable under different conditions or time points. It compares the means of related groups (within-subjects factor) and assesses the effects of both the within-subjects factor and any between-subjects factors.\n",
    "Example: Evaluating the effect of different exercise routines (condition: before vs. after) on cardiovascular fitness within the same group of participants."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b4c493e-37d4-400c-a537-04c6ed901e3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "38a00fd4-6ce9-4018-a03f-7fa1007f800f",
   "metadata": {},
   "source": [
    "Ques.3 What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61e8f365-13c5-466f-8940-18c182f5a049",
   "metadata": {},
   "source": [
    "In ANOVA (Analysis of Variance), the partitioning of variance refers to the division of the total variance observed in the data into different components that can be attributed to different sources or factors. Understanding this concept is crucial because it allows researchers to quantify and interpret the contributions of various factors to the overall variability in the dependent variable. This partitioning helps in assessing the significance of these factors and understanding their effects on the outcome variable.\n",
    "\n",
    "Components of Variance in ANOVA:\n",
    "\n",
    "Total Variance (Total Sum of Squares, SS_total):\n",
    "This represents the total variability in the dependent variable across all observations or groups\n",
    "\n",
    "Between-Group Variance (Between-Groups Sum of Squares, SS_between):\n",
    "This component measures the variability between different groups or levels of the independent variable. It reflects how much the group means differ from each other.\n",
    "\n",
    "Within-Group Variance (Within-Groups Sum of Squares, SS_within or SS_error):\n",
    "This component accounts for the variability within each group. It represents the differences between individual observations and their group mean.\n",
    "\n",
    "Importance of Understanding Partitioning of Variance:\n",
    "Identifying Sources of Variation:\n",
    "By partitioning variance, ANOVA helps identify which factors (or combinations of factors) contribute significantly to the differences observed in the dependent variable. This is crucial for understanding the main effects of each factor and any interactions between factors\n",
    "\n",
    "Assessing Significance:\n",
    "ANOVA calculates F-statistics based on the ratio of between-group variance to within-group variance. This ratio helps determine whether the observed differences between groups are statistically significant or simply due to random chance.\n",
    "\n",
    "Interpreting Results:\n",
    "Partitioning of variance provides insights into the relative importance of different factors in explaining the variability in the dependent variable. Researchers can interpret whether differences between groups are likely due to the independent variable(s) under study or due to random sampling variability.\n",
    "\n",
    "Guiding Further Analysis:\n",
    "Understanding how variance is partitioned can guide further analysis, such as post-hoc tests or subgroup analyses, to explore specific group differences or interactions that may be of interest.\n",
    "\n",
    "Improving Experimental Design:\n",
    "For experimental studies, understanding variance partitioning can inform the design of future experiments by identifying which factors to control or manipulate to minimize unwanted variability and enhance the ability to detect meaningful effects."
   ]
  },
  {
   "cell_type": "raw",
   "id": "35832b62-ccb6-4359-b589-c9efde790baf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b807ba14-d488-4632-96ed-c3a6574d52b8",
   "metadata": {},
   "source": [
    "Ques.4  How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f50ed-2151-443c-8b27-b24730853cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Example data (DV and IV)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate example data for three groups\n",
    "group1 = np.random.normal(10, 2, 20)\n",
    "group2 = np.random.normal(12, 2, 20)\n",
    "group3 = np.random.normal(15, 2, 20)\n",
    "\n",
    "# Combine data into a single array\n",
    "data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Define corresponding group labels\n",
    "labels = ['Group1'] * 20 + ['Group2'] * 20 + ['Group3'] * 20\n",
    "\n",
    "# Calculate overall mean of the data\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = []\n",
    "for group in [group1, group2, group3]:\n",
    "    group_means.append(np.mean(group))\n",
    "\n",
    "# Calculate SST (Total Sum of Squares)\n",
    "SST = np.sum((data - overall_mean)**2)\n",
    "\n",
    "# Calculate SSE (Explained Sum of Squares)\n",
    "SSE = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group1, group2, group3], group_means)])\n",
    "\n",
    "# Calculate SSR (Residual Sum of Squares)\n",
    "SSR = np.sum([(x - group_means[labels[i]])**2 for i, x in enumerate(data)])\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total Sum of Squares (SST): {SST}\")\n",
    "print(f\"Explained Sum of Squares (SSE): {SSE}\")\n",
    "print(f\"Residual Sum of Squares (SSR): {SSR}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcc047f-7310-40b3-b0e1-52f94320f33d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0245605c-bc0b-4dcf-a2fd-334f6dbc80c9",
   "metadata": {},
   "source": [
    "Ques.5 In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a126cc5e-036c-48ce-95f7-1792213a758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              sum_sq    df         F    PR(>F)\n",
      "C(Factor_A)               686.309084   2.0  5.922432  0.009128\n",
      "C(Factor_B)               218.989778   2.0  1.889749  0.175926\n",
      "C(Factor_A):C(Factor_B)   144.356529   4.0  0.622855  0.651294\n",
      "Residual                 1216.771236  21.0       NaN       NaN\n",
      "Main effect of Factor A: 343.1545419535651\n",
      "Main effect of Factor B: 109.4948891801507\n",
      "Interaction effect: 36.089132306337945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data (DV and two IVs)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate example data for two factors (IVs)\n",
    "factor_A = np.repeat(['A1', 'A2', 'A3'], 10)  # Factor A with 3 levels\n",
    "factor_B = np.tile(['B1', 'B2', 'B3'], 10)    # Factor B with 3 levels\n",
    "data = pd.DataFrame({\n",
    "    'Factor_A': factor_A,\n",
    "    'Factor_B': factor_B,\n",
    "    'DV': np.random.normal(50, 10, 30)  # Example DV data\n",
    "})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('DV ~ C(Factor_A) + C(Factor_B) + C(Factor_A):C(Factor_B)', data=data).fit()\n",
    "\n",
    "# Extract ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA table to see main effects and interaction effect\n",
    "print(anova_table)\n",
    "\n",
    "# Extract main effects and interaction effect\n",
    "main_effect_A = anova_table.loc['C(Factor_A)', 'sum_sq'] / anova_table.loc['C(Factor_A)', 'df']\n",
    "main_effect_B = anova_table.loc['C(Factor_B)', 'sum_sq'] / anova_table.loc['C(Factor_B)', 'df']\n",
    "interaction_effect = anova_table.loc['C(Factor_A):C(Factor_B)', 'sum_sq'] / anova_table.loc['C(Factor_A):C(Factor_B)', 'df']\n",
    "\n",
    "# Output the results\n",
    "print(f\"Main effect of Factor A: {main_effect_A}\")\n",
    "print(f\"Main effect of Factor B: {main_effect_B}\")\n",
    "print(f\"Interaction effect: {interaction_effect}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3321b014-1865-41f5-8874-22bf47a84a74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b2544a01-d063-4170-b86c-2a2757c534a5",
   "metadata": {},
   "source": [
    "Ques.6 Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would        you interpret these results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a11fa024-6f88-4349-aba9-a2898e979908",
   "metadata": {},
   "source": [
    "When conducting a one-way ANOVA, the F-statistic and its associated p-value are used to determine whether there are statistically significant differences in the means of the groups being compared. Here’s how to interpret the results given:\n",
    "\n",
    "F-Statistic: The F-statistic is a ratio of the between-group variability to within-group variability. It quantifies whether the differences in means between groups are greater than what would be expected due to random chance.\n",
    "\n",
    "P-Value: The p-value associated with the F-statistic indicates the probability of obtaining an F-statistic as extreme as observed (or more extreme) under the assumption that there are no true differences between group means (i.e., the null hypothesis is true).\n",
    "\n",
    "Interpretation of the Results:\n",
    "F-Statistic (5.23): This value indicates that there is a moderate amount of variation between the group means relative to the variation within each group.\n",
    "P-Value (0.02): This p-value suggests that there is a 2% probability of observing such an extreme F-statistic if there were actually no differences between the group means (i.e., if the null hypothesis were true).\n",
    "\n",
    "Conclusion:\n",
    "Based on the F-statistic of 5.23 and the p-value of 0.02:\n",
    "Statistical Significance: Since the p-value (0.02) is less than the significance level (commonly 0.05), we reject the null hypothesis. This means that we have sufficient evidence to conclude that there are statistically significant differences in the means of the groups.\n",
    "Group Differences: The differences observed between the group means are unlikely to be due to random chance alone. Therefore, there are likely true differences in the population means of the groups being studied.\n",
    "\n",
    "Practical Interpretation:\n",
    "Practical Significance: While statistical significance indicates that there are differences between the groups, it is also important to consider whether these differences are practically significant or meaningful in the context of the study.\n",
    "Further Analysis: Post-hoc tests (such as Tukey's HSD, Bonferroni, or LSD tests) can be conducted to determine which specific groups differ significantly from each other."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d586bfe-865c-4caf-bade-9c8f474472f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c8584fc6-49ce-46c0-a45e-e8aa36db6226",
   "metadata": {},
   "source": [
    "Ques.7  In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0309778d-7d03-4554-874a-6615174737cb",
   "metadata": {},
   "source": [
    "Handling Missing Data in Repeated Measures ANOVA:\n",
    "\n",
    "Identify and Understand Missing Data:\n",
    "First, identify the pattern and reasons for missing data. It could be missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Understanding this can guide your choice of handling method.\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion):\n",
    "This approach involves excluding cases with missing data on any of the variables involved in the analysis. It's straightforward but reduces sample size and potentially biases results if missingness is not random.\n",
    "\n",
    "Imputation Methods:\n",
    "Mean/Mode/Median Imputation: Replace missing values with the mean, mode, or median of the observed values for that variable. Simple but can distort variance and correlations.\n",
    "Last Observation Carried Forward (LOCF): Use the last observed value for subsequent missing values. Suitable for data with a time trend but may underestimate variability.\n",
    "Multiple Imputation: Generate multiple plausible values for each missing data point to account for uncertainty. Requires assumptions about the distribution of missing data and is computationally intensive.\n",
    "Regression Imputation: Predict missing values based on other variables using regression models. Can be effective but assumes relationships among variables are correctly specified\n",
    "\n",
    "Model-Based Methods:\n",
    "Use statistical models to handle missing data, such as mixed-effects models (for longitudinal data) or Bayesian methods. These methods can account for missing data patterns and provide more accurate estimates.\n",
    "\n",
    "Potential Consequences of Different Methods:\n",
    "Bias: Some methods (like mean imputation) can introduce bias if missing data are not MCAR.\n",
    "Loss of Power: Complete case analysis reduces sample size, potentially decreasing the power to detect true effects.\n",
    "Distorted Variance Estimates: Imputation methods that do not properly account for variability in missing data can underestimate standard errors and inflate type I error rates.\n",
    "Invalid Assumptions: Model-based methods rely on assumptions about the missing data mechanism (MCAR, MAR) and model specification. Violations of these assumptions can lead to incorrect inferences.\n",
    "\n",
    "Best Practices:\n",
    "Understand the Missing Data Mechanism: Use diagnostics to determine if missing data assumptions hold (e.g., Little's test).\n",
    "Transparent Reporting: Clearly document the handling method used and discuss potential implications for interpretation.\n",
    "Sensitivity Analysis: Perform sensitivity analyses using different handling methods to assess robustness of results.\n",
    "\n",
    "Example Application:\n",
    "If conducting a repeated measures ANOVA on longitudinal data where some participants have missing measurements at certain time points, you might consider using multiple imputation to account for uncertainty and variability due to missing data patterns. This approach can provide more accurate estimates of treatment effects and variability compared to simple methods like complete case analysis or mean imputation.\n",
    "In conclusion, handling missing data in repeated measures ANOVA requires careful consideration of the missing data mechanism and choice of appropriate handling method to minimize bias and maximize the reliability of study findings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e22e05c4-ac57-42a6-8788-7952a339e426",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dc0740b7-7e4c-4707-829b-5f3ecc1f6e90",
   "metadata": {},
   "source": [
    "Ques.8 What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee9f2ed-1cbd-4362-b63d-7043e5432923",
   "metadata": {},
   "source": [
    "Common Post-Hoc Tests:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD) Test:\n",
    "Usage: Tukey's HSD test is widely used when you have conducted a one-way ANOVA and want to compare all possible pairs of means to determine which pairs are significantly different from each other.\n",
    "Example: Suppose you conducted an ANOVA to compare the effectiveness of four different teaching methods on student test scores. After finding a significant result, Tukey's HSD can be used to identify which specific pairs of teaching methods have significantly different mean test scores.\n",
    "\n",
    "Bonferroni Correction:\n",
    "Usage: Bonferroni correction adjusts the significance level (alpha) for each comparison to maintain an overall alpha level. It is often used when conducting multiple pairwise comparisons to control the family-wise error rate.\n",
    "Example: In genetics, researchers might conduct ANOVA to compare the expression levels of multiple genes across different experimental conditions. Bonferroni correction would be applied to determine which specific pairs of genes have significantly different expression levels.\n",
    "\n",
    "Scheffe's Test:\n",
    "Usage: Scheffe's test is a conservative post-hoc test that is used when the sample sizes are unequal or when the assumptions of equal variances and/or normality may be violated.\n",
    "Example: Suppose you conducted a study investigating the effects of three different types of therapy on anxiety levels in patients, where the sample sizes across therapies are not equal. Scheffe's test would be appropriate to determine which therapies lead to significantly different anxiety reduction.\n",
    "\n",
    "Duncan's New Multiple Range Test:\n",
    "Usage: Duncan's test compares means in a stepwise manner, starting with the groups with the largest differences. It is less conservative than Tukey's HSD but more powerful under certain conditions.\n",
    "Example: In agriculture, ANOVA might be used to compare crop yields across different fertilizer treatments. Duncan's test would help identify which specific pairs of fertilizer treatments lead to significantly different crop yields.\n",
    "\n",
    "When to Use Post-Hoc Tests:\n",
    "Significant ANOVA Result: Post-hoc tests are used after obtaining a significant result from ANOVA, indicating that there are differences among at least some of the group means.\n",
    "Multiple Group Comparisons: When there are more than two groups being compared, post-hoc tests are necessary to determine which specific pairs of groups differ significantly from each other.\n",
    "Interpreting Specific Differences: Post-hoc tests provide detailed information on which groups are significantly different, allowing for a more nuanced interpretation of study findings."
   ]
  },
  {
   "cell_type": "raw",
   "id": "82a4e7fa-652b-4322-bc8f-ecabee097b15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4db87984-9df1-45cf-8cbe-6396c442267a",
   "metadata": {},
   "source": [
    "Ques.9  A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets.               Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-             value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea86da7-b730-4eae-b764-152bda2435be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA results:\n",
      "F-statistic: 2.1998458860662016\n",
      "P-value: 0.11445565766520953\n",
      "The one-way ANOVA result is not significant, indicating that there are no significant differences in mean weight loss between the diets (A, B, and C).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data (mean weight loss for each diet)\n",
    "diet_A = np.random.normal(5, 1, 50)  # Mean weight loss for diet A\n",
    "diet_B = np.random.normal(4.5, 1.2, 50)  # Mean weight loss for diet B\n",
    "diet_C = np.random.normal(4.8, 1.1, 50)  # Mean weight loss for diet C\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(f\"One-way ANOVA results:\")\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA result is significant, indicating that there are likely significant differences \"\n",
    "          \"in mean weight loss between at least two of the diets (A, B, or C).\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA result is not significant, indicating that there are no significant differences \"\n",
    "          \"in mean weight loss between the diets (A, B, and C).\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98bff126-cbf5-459a-b396-7bf0c397470e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3fb22b6d-67e4-4110-8766-3cbb4c1c76d8",
   "metadata": {},
   "source": [
    "Ques.10 A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program         B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using             Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-         statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb40837-15f8-4cb0-8b40-9822e6875f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-way ANOVA results:\n",
      "                              sum_sq    df         F    PR(>F)\n",
      "C(Software)                11.141545   2.0  2.113814  0.142706\n",
      "C(Experience)               2.102143   1.0  0.797652  0.380665\n",
      "C(Software):C(Experience)   6.013261   2.0  1.140857  0.336272\n",
      "Residual                   63.249921  24.0       NaN       NaN\n",
      "\n",
      "Interpretation:\n",
      "There is no significant interaction effect between software programs and experience level.\n",
      "There is no significant main effect of software programs.\n",
      "There is no significant main effect of experience level.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data generation\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate example data\n",
    "n = 30\n",
    "software_programs = np.random.choice(['Program A', 'Program B', 'Program C'], n)\n",
    "experience_levels = np.random.choice(['Novice', 'Experienced'], n)\n",
    "times = np.random.normal(10, 2, n)  # Example times, normally distributed\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Software': software_programs,\n",
    "    'Experience': experience_levels,\n",
    "    'Time': times\n",
    "})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=data).fit()\n",
    "\n",
    "# Print ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(\"Two-way ANOVA results:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_table.loc['C(Software):C(Experience)', 'PR(>F)'] < alpha:\n",
    "    print(\"There is a significant interaction effect between software programs and experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between software programs and experience level.\")\n",
    "    \n",
    "if anova_table.loc['C(Software)', 'PR(>F)'] < alpha:\n",
    "    print(\"There is a significant main effect of software programs.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of software programs.\")\n",
    "    \n",
    "if anova_table.loc['C(Experience)', 'PR(>F)'] < alpha:\n",
    "    print(\"There is a significant main effect of experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of experience level.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c85d79d-1b02-4621-bc82-4ae9c400f2a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b76dc6a9-2132-4d2d-8bca-ac5a26b32b7a",
   "metadata": {},
   "source": [
    "Ques.11  An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group                  (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to          determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which          group(s) differ significantly from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4110306f-ffb2-41d1-a0b4-5b3ee99f60e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "T-statistic: -1.6677351961320235\n",
      "P-value: 0.09856078338184605\n",
      "The two-sample t-test result is not significant, indicating that there is no significant difference in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example data (test scores)\n",
    "np.random.seed(0)\n",
    "\n",
    "control_scores = np.random.normal(70, 10, 50)   # Control group (traditional method)\n",
    "experimental_scores = np.random.normal(75, 10, 50)  # Experimental group (new method)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print t-test results\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret the t-test results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"The two-sample t-test result is significant, indicating that there is a significant difference \"\n",
    "          \"in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"The two-sample t-test result is not significant, indicating that there is no significant difference \"\n",
    "          \"in test scores between the control and experimental groups.\")\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test if t-test is significant\n",
    "if p_value < alpha:\n",
    "    data = np.concatenate([control_scores, experimental_scores])\n",
    "    groups = ['Control'] * 50 + ['Experimental'] * 50\n",
    "    tukey_results = pairwise_tukeyhsd(data, groups, alpha=0.05)\n",
    "    print(\"\\nTukey's HSD post-hoc test results:\")\n",
    "    print(tukey_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8986863a-6533-4758-8e9c-715630628475",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "906b5dfe-3613-48e2-9ac4-e1659a2139db",
   "metadata": {},
   "source": [
    "Ques.12  A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select            30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales              between the three stores. If the results are significant, follow up with a post\u0002hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1f9ffa-8836-4c37-8110-d5d10f089a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated measures ANOVA results:\n",
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store  1.5049 2.0000 58.0000 0.2306\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example data generation\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate example data\n",
    "days = np.arange(1, 31)\n",
    "store_A_sales = np.random.normal(1000, 100, 30)  # Example sales for Store A\n",
    "store_B_sales = np.random.normal(1100, 120, 30)  # Example sales for Store B\n",
    "store_C_sales = np.random.normal(1050, 110, 30)  # Example sales for Store C\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Day': np.repeat(days, 3),\n",
    "    'Store': np.tile(['A', 'B', 'C'], 30),\n",
    "    'Sales': np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n",
    "})\n",
    "\n",
    "# Convert 'Day' and 'Store' to categorical variables\n",
    "data['Day'] = pd.Categorical(data['Day'])\n",
    "data['Store'] = pd.Categorical(data['Store'])\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "anova_rm = AnovaRM(data, 'Sales', 'Day', within=['Store'])\n",
    "results = anova_rm.fit()\n",
    "\n",
    "# Print ANOVA table\n",
    "print(\"Repeated measures ANOVA results:\")\n",
    "print(results)\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test if ANOVA is significant\n",
    "if results.anova_table['Pr > F'][0] < 0.05:  # Check p-value for the 'Store' factor\n",
    "    tukey_results = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=0.05)\n",
    "    print(\"\\nTukey's HSD post-hoc test results:\")\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35f204-34b8-4355-bc0a-69a72e285949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
